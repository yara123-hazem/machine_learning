{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Imported Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Shared Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"shared_variables.pkl\", \"rb\") as f:\n",
    "    columns = pickle.load(f)\n",
    "    X_train = pickle.load(f)\n",
    "    y_train = pickle.load(f)\n",
    "    X_test = pickle.load(f)\n",
    "    y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Models\n",
    "    1. Bagging (With Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.6676294528862656\n"
     ]
    }
   ],
   "source": [
    "# Function to implement bagging with Logistic Regression and grid search\n",
    "def bagging(X_train, y_train, num_models, param_grid):\n",
    "    models = []\n",
    "\n",
    "    for _ in range(num_models):\n",
    "        # Randomly sample with replacement\n",
    "        indices = np.random.choice(len(X_train), size=len(X_train), replace=True)\n",
    "        X_subset = X_train.iloc[indices]\n",
    "        y_subset = y_train.iloc[indices]\n",
    "        \n",
    "        base_model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "        grid_search = GridSearchCV(base_model, param_grid, scoring='accuracy', cv=5)\n",
    "        grid_search.fit(X_subset, y_subset)\n",
    "\n",
    "        # Use the best estimator from grid search\n",
    "        best_model = grid_search.best_estimator_\n",
    "    \n",
    "        models.append(best_model)\n",
    "\n",
    "    return models\n",
    "\n",
    "# Function to make predictions using bagging\n",
    "def predict_bagging(models, X_test):\n",
    "    predictions = np.zeros((len(X_test), len(models)))  \n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        predictions[:, i] = model.predict(X_test)\n",
    "\n",
    "    # Use majority voting for classification\n",
    "    ensemble_predictions = np.apply_along_axis(lambda x: np.bincount(x.astype(int)).argmax(), \n",
    "                                               axis=1, arr=predictions)\n",
    "\n",
    "    return ensemble_predictions\n",
    "\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'solver': ['lbfgs', 'liblinear']}\n",
    "\n",
    "# Train bagging ensemble\n",
    "num_models = 10 \n",
    "models = bagging(X_train, y_train, num_models, param_grid)\n",
    "\n",
    "# Make predictions\n",
    "ensemble_predictions = predict_bagging(models, X_test)\n",
    "\n",
    "# Evaluate Accuracy\n",
    "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(f\"Accuracy on test data: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.5517183641006321\n"
     ]
    }
   ],
   "source": [
    "def initialize_weights(n_samples):\n",
    "    return np.ones(n_samples) / n_samples\n",
    "\n",
    "def update_weights(weights, alpha, y_true, y_pred):\n",
    "    incorrect = (y_true != y_pred).astype(int)\n",
    "    updated_weights = weights * np.exp(alpha * incorrect)\n",
    "    return updated_weights / np.sum(updated_weights)\n",
    "\n",
    "def adaboost(X, y, n_estimators):\n",
    "    n_samples, n_features = X.shape \n",
    "    weights = initialize_weights(n_samples)\n",
    "    models = []\n",
    "    alphas = []\n",
    "\n",
    "    for _ in range(n_estimators):\n",
    "        # Train a weak learner (Decision Tree in this case)\n",
    "        model = DecisionTreeClassifier(max_depth=1)\n",
    "        model.fit(X, y, sample_weight=weights)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        # Calculate error and alpha\n",
    "        error = np.sum(weights * (y_pred != y)) / np.sum(weights)\n",
    "        alpha = 0.5 * np.log((1 - error) / error)\n",
    "\n",
    "        # Update weights\n",
    "        weights = update_weights(weights, alpha, y, y_pred)\n",
    "\n",
    "        # Save the model and alpha\n",
    "        models.append(model)\n",
    "        alphas.append(alpha)\n",
    "\n",
    "    return models, alphas\n",
    "\n",
    "def adaboost_predict(models, alphas, X):\n",
    "    n_samples = X.shape[0]\n",
    "    predictions = np.zeros(n_samples)\n",
    "\n",
    "    for model, alpha in zip(models, alphas):\n",
    "        predictions += alpha * model.predict(X)\n",
    "\n",
    "    return np.sign(predictions)\n",
    "\n",
    "# Train AdaBoost\n",
    "n_estimators = 5\n",
    "models, alphas = adaboost(X_train, y_train, n_estimators)\n",
    "\n",
    "# Make predictions on test data\n",
    "y_pred_train = adaboost_predict(models, alphas, X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_train)\n",
    "print(f\"Accuracy on test data: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.6297877684289841\n"
     ]
    }
   ],
   "source": [
    "def bootstrap_sample(data, labels):\n",
    "    n_samples = len(data)\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    return data.iloc[indices], labels.iloc[indices]\n",
    "\n",
    "def random_forest(X, y, n_estimators, max_features=None):\n",
    "    #n_samples, n_features = X.shape\n",
    "    models = []\n",
    "\n",
    "    for i in range(n_estimators):\n",
    "        # Create a bootstrap sample\n",
    "        bootstrap_data, bootstrap_labels = bootstrap_sample(X, y)\n",
    "\n",
    "        # Train a decision tree on the bootstrap sample\n",
    "        model = DecisionTreeClassifier(max_features=max_features)\n",
    "        model.fit(bootstrap_data, bootstrap_labels)\n",
    "\n",
    "        # Add the trained model to the ensemble\n",
    "        models.append(model)\n",
    "\n",
    "    return models\n",
    "\n",
    "def random_forest_predict(models, X):\n",
    "    predictions = np.zeros((X.shape[0], len(models)))\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        predictions[:, i] = model.predict(X)\n",
    "\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x.astype(int)).argmax(), \n",
    "                                            axis=1, arr=predictions)\n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "n_estimators = 5\n",
    "max_features = 'sqrt'\n",
    "models = random_forest(X_train, y_train, n_estimators, max_features)\n",
    "\n",
    "y_pred_test = random_forest_predict(models, X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy on test data: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Random Forests (With BayesSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.int = int\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "def bootstrap_sample(data, labels):\n",
    "    n_samples = len(data)\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    return data.iloc[indices], labels.iloc[indices]\n",
    "\n",
    "def random_forest(X, y, n_estimators, max_features=None, param_space=None):\n",
    "    #n_samples, n_features = X.shape\n",
    "    models = []\n",
    "\n",
    "    for i in range(n_estimators):\n",
    "        # Create a bootstrap sample\n",
    "        bootstrap_data, bootstrap_labels = bootstrap_sample(X, y)\n",
    "\n",
    "        # Train a decision tree on the bootstrap sample\n",
    "        base_model = DecisionTreeClassifier(max_features=max_features)\n",
    "\n",
    "        # Define the Bayesian optimization object\n",
    "        opt = BayesSearchCV(base_model, param_space, n_iter=50, cv=5, n_jobs=-1)\n",
    "\n",
    "        # Fit the Bayesian optimization object to the training data\n",
    "        opt.fit(bootstrap_data, bootstrap_labels)\n",
    "\n",
    "        # Get the best hyperparameters\n",
    "        best_model = opt.best_estimator_\n",
    "\n",
    "        # Add the trained model to the ensemble\n",
    "        models.append(best_model)\n",
    "\n",
    "    return models\n",
    "\n",
    "def random_forest_predict(models, X):\n",
    "    predictions = np.zeros((X.shape[0], len(models)))\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        predictions[:, i] = model.predict(X)\n",
    "\n",
    "    final_predictions = np.apply_along_axis(lambda x: np.bincount(x.astype(np.int64)).argmax(), axis=1, arr=predictions)\n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "param_space = {          \n",
    "    'max_depth': (1, 20),               # Maximum depth of the trees\n",
    "    'min_samples_split': (2, 10),       # Minimum samples required to split an internal node\n",
    "    'min_samples_leaf': (1, 10),        # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "n_estimators = 5\n",
    "max_features = 'sqrt'\n",
    "models = random_forest(X_train, y_train, n_estimators, max_features, param_space)\n",
    "\n",
    "y_pred_test = random_forest_predict(models, X_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Accuracy on test data: {accuracy_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
